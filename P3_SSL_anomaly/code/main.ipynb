{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-supervised Learning\n",
    "The objective of this lab project is to go further in the understanding of Self-Supervised Learning (SSL). By the end of the notebook, you will\n",
    "- Train models using different pretext tasks: colorizing, inpainting, masking reconstruction.\n",
    "- Fine-tune the models with the downstream task of interst.\n",
    "- Compare the performance of the different backbones obtained from the different downstream tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretext and Downstream Tasks\n",
    "\n",
    "We will train three different models using three different pretext tasks. All three models will be trained on the SVHN dataset. The three models are the following:\n",
    "- A Colorization Neural Network\n",
    "- An Inpainting Neural Network\n",
    "- A Masked Autoencoder\n",
    "\n",
    "We will build a common architecture so that all three models have as similar architectures as possible. The common architecture will consist of an encoder and a decoder. Once the models pre-trained on their respective pretext tasks, we will use the pre-trained encoders to evaluate the learnt representations on two new downstream task: image classification on MNIST and on SVHN. To that end, we will perform a linear evaluation protocol, by freezing the weights of the pre-trained encoders, and training a linear classifier on the learnt representations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "RESIZE = 64\n",
    "LATENT_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clonage dans '../data/mvtec'...\n",
      "remote: Enumerating objects: 1287, done.\u001b[K\n",
      "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
      "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
      "remote: Total 1287 (delta 2), reused 0 (delta 0), pack-reused 1275 (from 1)\u001b[K\n",
      "Réception d'objets: 100% (1287/1287), 60.20 Mio | 9.03 Mio/s, fait.\n",
      "Résolution des deltas: 100% (2/2), fait.\n",
      "Clonage dans '../data/ew'...\n",
      "remote: Enumerating objects: 857, done.\u001b[K\n",
      "remote: Counting objects: 100% (857/857), done.\u001b[K\n",
      "remote: Compressing objects: 100% (855/855), done.\u001b[K\n",
      "remote: Total 857 (delta 3), reused 842 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
      "Réception d'objets: 100% (857/857), 15.80 Mio | 7.69 Mio/s, fait.\n",
      "Résolution des deltas: 100% (3/3), fait.\n",
      "Dossiers téléchargés dans le dossier '/home/hp/Scolarite/INSA/5A/HDDL/Projets_HDDL/P3_SSL_anomaly/code/../data/'.\n"
     ]
    }
   ],
   "source": [
    "# Crée un chemin relatif basé sur le dossier actuel\n",
    "dataset_path = os.path.join(os.getcwd(), '../data/')\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path, exist_ok=True)  # Crée le dossier et ses parents si nécessaire\n",
    "\n",
    "    !git clone https://github.com/sourisimos/DATA_P3HDDL_MVTEC.git ../data/mvtec #Cloning mvtec dataset \n",
    "    !git clone https://github.com/sourisimos/DATA_P3HDDL_EW.git ../data/ew #Cloning ew datasets\n",
    "\n",
    "    print(f\"Dossiers téléchargés dans le dossier '{dataset_path}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine wiring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations to apply to EW\n",
    "\n",
    "transform_ew_val = transforms.Compose([\n",
    "    transforms.Resize((RESIZE, RESIZE)),\n",
    "    transforms.ToTensor()\n",
    "\n",
    "])\n",
    "\n",
    "transform_ew_train = transforms.Compose([\n",
    "    transforms.Resize((RESIZE,RESIZE)),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "\n",
    "# Load EW dataset for train test and creating val\n",
    "\n",
    "# Training\n",
    "ew_train_dataset = datasets.ImageFolder(root='../data/engine_wiring/train', transform=transform_ew_train)\n",
    "ew_train_loader = DataLoader(ew_train_dataset, batch_size=BATCH_SIZE, shuffle=True )\n",
    "\n",
    "# Test\n",
    "ew_test_dataset = datasets.ImageFolder(root='../data/engine_wiring/test_merge', transform=transform_ew_val) \n",
    "ew_test_loader = DataLoader(ew_test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVtec datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations to apply to the images\n",
    "transform_mvtec = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "\n",
    "# Function to load the dataset mvtec \n",
    "def load_mvt_dataset(root='../data/mvtec_anomaly_detection', batch_size=64, restricted = True):\n",
    "    # List of categories you want to load\n",
    "    categories = ['bottle', 'cable','capsule', 'carpet', 'grid','hazelnut', 'leather', 'metal_nut', 'pill', 'screw', 'tile', 'toothbrush', 'transistor', 'wood', 'zipper']  # Add other categories as needed\n",
    "    # Restricted are just the essential one (for the academ project )\n",
    "    if restricted: \n",
    "        categories = ['bottle','capsule', 'hazelnut', 'toothbrush']\n",
    "        \n",
    "    # Create dictionaries to hold loaders for training and test sets\n",
    "    train_loaders = {}\n",
    "    test_loaders = {}\n",
    "    \n",
    "    for category in categories:\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(root=f'{root}/{category}/train', transform=transform_mvtec)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        train_loaders[category] = train_loader\n",
    "        \n",
    "        test_dataset = datasets.ImageFolder(root=f'{root}/{category}/test', transform=transform_mvtec)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        test_loaders[category] = test_loader\n",
    "        \n",
    "    return train_loaders, test_loaders\n",
    "\n",
    "train_loaders, test_loaders = load_mvt_dataset()\n",
    "\n",
    "# Accessing a specific category's data loader\n",
    "bottle_train_loader = train_loaders['bottle']\n",
    "bottle_test_loader = test_loaders['bottle']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared architecture\n",
    "We will create a shared architecture with the following layers:\n",
    "- The encoder (in sequential order):\n",
    "    - A convolution with 64 filters, kernel size 4, stride 2, padding 1\n",
    "    - A ReLU activation\n",
    "    - A convolution with 128 filters, kernel size 4, stride2, padding 1\n",
    "    - A ReLU activation\n",
    "    - A convolution with `latent_dim`, kernel size 4, stride 2, padding 1\n",
    "    - A ReLU activation\n",
    "The encoder should take the number of channels of the input `in_channels` and the hidden dimension `latent_dim` as arguments.\n",
    "- The decoder (in secuential ) order:\n",
    "    - A Transpose convolution with 128 filters, kernel size 4, padding 1\n",
    "    - A ReLU activation\n",
    "    - A convolution with 16428 filters, kernel size 4, stride2, padding 1\n",
    "    - A ReLU activation\n",
    "    - A convolution with `out_channels` filters, kernel size 4, stride 2, padding 1\n",
    "    - A ReLU activation\n",
    "The encoder should take the number of channels of the output `out_channels` and the hidden dimension `latent_dim` as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, latent_dim=32):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 8x8 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, latent_dim, kernel_size=4, stride=2, padding=1),  # 4x4 -> 2x2\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=32, out_channels=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 256, kernel_size=4, stride=2, padding=1),  # 2x2 -> 4x4\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 4x4 -> 8x8\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 8x8 -> 16x16\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1),  # 16x16 -> 32x32\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ssl_model(model, \n",
    "                    train_loader, \n",
    "                    val_loader, \n",
    "                    criterion,\n",
    "                    optimizer,\n",
    "                    device=device,\n",
    "                    epochs=5):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(images)\n",
    "            loss = criterion(output, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, _ in val_loader:\n",
    "                images = images.to(device)\n",
    "                output, _ = model(images)\n",
    "                val_loss = criterion(output, images)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "\n",
    "    return model.encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorization model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a de grande chance d'être nul dans le cadre de nos données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationModel(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        super(ColorizationModel, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, in_channels=1)  # Input grayscale\n",
    "        self.decoder = Decoder(latent_dim=latent_dim, out_channels=3)  # Predict RGB\n",
    "\n",
    "    def forward(self, x):\n",
    "        grayscale_x = transforms.Grayscale()(x)  # Convert RGB to Grayscale\n",
    "        z = self.encoder(grayscale_x)\n",
    "        return self.decoder(z), grayscale_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.1077, Avg Val Loss: 0.0969\n",
      "Epoch 2/5, Train Loss: 0.1045, Avg Val Loss: 0.0964\n",
      "Epoch 3/5, Train Loss: 0.1033, Avg Val Loss: 0.0808\n",
      "Epoch 4/5, Train Loss: 0.0835, Avg Val Loss: 0.0680\n",
      "Epoch 5/5, Train Loss: 0.0663, Avg Val Loss: 0.0548\n"
     ]
    }
   ],
   "source": [
    "colorization_model = ColorizationModel(latent_dim=LATENT_DIM)\n",
    "colorization_encoder = train_ssl_model(colorization_model, \n",
    "                                       ew_train_loader, \n",
    "                                       ew_test_loader, \n",
    "                                       criterion=nn.MSELoss(), \n",
    "                                       optimizer=optim.Adam(colorization_model.parameters(), lr=0.001)\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InpaintingModel(nn.Module):\n",
    "    def __init__(self, latent_dim=128, mask_size=8):\n",
    "        super(InpaintingModel, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim)\n",
    "        self.mask_size = mask_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_masked = self.apply_mask(x)\n",
    "        z = self.encoder(x_masked)\n",
    "        return self.decoder(z), x_masked\n",
    "    \n",
    "    def apply_mask(self, x):\n",
    "        masked_x = x.clone()\n",
    "\n",
    "        for i in range(masked_x.size(0)):\n",
    "            ul_x = np.random.randint(0, x.size(2) - self.mask_size + 1)\n",
    "            ul_y = np.random.randint(0, x.size(3) - self.mask_size + 1)\n",
    "            masked_x[i, :, ul_x:ul_x+self.mask_size, ul_y:ul_y+self.mask_size] = 0\n",
    "\n",
    "        return masked_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.1058, Avg Val Loss: 0.0977\n",
      "Epoch 2/5, Train Loss: 0.1055, Avg Val Loss: 0.0902\n",
      "Epoch 3/5, Train Loss: 0.0914, Avg Val Loss: 0.0632\n",
      "Epoch 4/5, Train Loss: 0.0896, Avg Val Loss: 0.0764\n",
      "Epoch 5/5, Train Loss: 0.0792, Avg Val Loss: 0.0642\n"
     ]
    }
   ],
   "source": [
    "inpainting_model = InpaintingModel(latent_dim=LATENT_DIM, mask_size=8)\n",
    "inpainting_encoder = train_ssl_model(inpainting_model,\n",
    "                                     ew_train_loader, \n",
    "                                     ew_test_loader, \n",
    "                                     criterion=nn.MSELoss(), \n",
    "                                     optimizer=optim.Adam(inpainting_model.parameters(), lr=0.001)\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked AE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedAutoencoderModel(nn.Module):\n",
    "    def __init__(self, latent_dim=32, mask_ratio=1/16):\n",
    "        super(MaskedAutoencoderModel, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim=latent_dim)\n",
    "        self.decoder = Decoder(latent_dim=latent_dim)\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_masked = self.apply_mask(x)\n",
    "        z = self.encoder(x_masked)\n",
    "        return self.decoder(z), x_masked\n",
    "    \n",
    "    def apply_mask(self, x):\n",
    "        x_masked = x.clone()\n",
    "        mask = torch.rand_like(x[:, 0, :, :]) < self.mask_ratio\n",
    "        mask = mask.unsqueeze(1).repeat(1, x.size(1), 1, 1)\n",
    "        x_masked[mask] = 0\n",
    "        return x_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Train Loss: 0.1073, Avg Val Loss: 0.1054\n",
      "Epoch 2/5, Train Loss: 0.1008, Avg Val Loss: 0.0960\n",
      "Epoch 3/5, Train Loss: 0.0888, Avg Val Loss: 0.0772\n",
      "Epoch 4/5, Train Loss: 0.0750, Avg Val Loss: 0.0640\n",
      "Epoch 5/5, Train Loss: 0.0635, Avg Val Loss: 0.0632\n"
     ]
    }
   ],
   "source": [
    "mae_model = MaskedAutoencoderModel(latent_dim=LATENT_DIM, mask_ratio=1/16)\n",
    "mae_encoder = train_ssl_model(mae_model,\n",
    "                              ew_train_loader, \n",
    "                              ew_train_loader, \n",
    "                              criterion=nn.MSELoss(), \n",
    "                              optimizer=optim.Adam(mae_model.parameters(), lr=0.001)\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Contrastive Dataset\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get original image\n",
    "        img, _ = self.dataset[idx]\n",
    "        # Create positive sample through augmentation\n",
    "        return img.float(), img.float()\n",
    "\n",
    "\n",
    "# Define the Projection Head\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Define the Contrastive Loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        z1 = nn.functional.normalize(z1, dim=-1)\n",
    "        z2 = nn.functional.normalize(z2, dim=-1)\n",
    "        similarity = torch.mm(z1, z2.T) / self.temperature\n",
    "        labels = torch.arange(z1.size(0)).to(z1.device)\n",
    "        loss = nn.functional.cross_entropy(similarity, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for contrastive learning\n",
    "def train_contrastive_model(encoder, projection_head, \n",
    "                    criterion,\n",
    "                    optimizer, train_loader, val_loader, epochs=5):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        projection_head.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for img1, img2 in train_loader:\n",
    "            img1, img2 = img1.to(device), img2.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            z1 = encoder(img1)\n",
    "            z1 = projection_head(z1.view(z1.size(0), -1))  # Aplatir pour la tête de projection\n",
    "            z2 = encoder(img2)\n",
    "            z2 = projection_head(z2.view(z2.size(0), -1))\n",
    "\n",
    "            loss = criterion(z1, z2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        encoder.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, _ in val_loader:\n",
    "                images = images.to(device)\n",
    "                output, _ = encoder(images)\n",
    "                val_loss = criterion(output, images)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m contrastive_loss \u001b[38;5;241m=\u001b[39m ContrastiveLoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mlist\u001b[39m(encoder\u001b[38;5;241m.\u001b[39mparameters()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(projection_head\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_contrastive_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprojection_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrastive_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mew_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mew_test_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[110], line 32\u001b[0m, in \u001b[0;36mtrain_contrastive_model\u001b[0;34m(encoder, projection_head, criterion, optimizer, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, _ \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[1;32m     31\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 32\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m encoder(images)\n\u001b[1;32m     33\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m criterion(output, images)\n\u001b[1;32m     34\u001b[0m     total_val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m val_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Train the contrastive model\n",
    "\n",
    "encoder = Encoder(in_channels=3, latent_dim=LATENT_DIM).to(device)\n",
    "projection_head = ProjectionHead(input_dim=LATENT_DIM * 4 * 4, output_dim=64).to(device)\n",
    "contrastive_loss = ContrastiveLoss().to(device)\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(projection_head.parameters()), lr=1e-3)\n",
    "\n",
    "train_contrastive_model(encoder, projection_head, contrastive_loss, optimizer ,ew_train_loader, ew_test_loader, epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Anomaly_detector(nn.Module):\n",
    "    def __init__(self, encoder, num_classes=2):\n",
    "        super(Anomaly_detector, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc = nn.Linear(4*4*LATENT_DIM, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        z = z.view(z.size(0), -1)\n",
    "        return self.fc(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning loop for classification\n",
    "def fine_tune_ew(encoder, train_loader, test_loader, epochs=5, encoder_in_channels=3):\n",
    "    \n",
    "    model = Anomaly_detector(encoder).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Freeze the encoder's weights\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            if encoder_in_channels == 1:\n",
    "                images = transforms.Grayscale()(images)  # Convert RGB to Grayscale\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            # print('output:', output)\n",
    "            # print(np.shape(output))\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            # print(\"\\n predicted\", predicted, '\\n')\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "    \n",
    "    \n",
    "    # Evaluate on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            if encoder_in_channels == 1:\n",
    "                images = transforms.Grayscale()(images)  # Convert RGB to Grayscale\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            output = model(images)\n",
    "            # print(output)\n",
    "            # print(np.shape(output))\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            print(predicted)\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.18128189941247305, Accuracy: 100.00%\n",
      "Epoch 2/5, Loss: 0.16985451181729636, Accuracy: 100.00%\n",
      "Epoch 3/5, Loss: 0.16324039796988168, Accuracy: 100.00%\n",
      "Epoch 4/5, Loss: 0.14742258687814078, Accuracy: 100.00%\n",
      "Epoch 5/5, Loss: 0.13639810432990393, Accuracy: 100.00%\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Test Accuracy: 53.05%\n"
     ]
    }
   ],
   "source": [
    "fine_tune_ew(colorization_encoder, ew_train_loader, ew_test_loader, encoder_in_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.5533777872721354, Accuracy: 62.11%\n",
      "Epoch 2/5, Loss: 0.4865899582703908, Accuracy: 61.40%\n",
      "Epoch 3/5, Loss: 0.48490514357884723, Accuracy: 63.16%\n",
      "Epoch 4/5, Loss: 0.5186337828636169, Accuracy: 69.47%\n",
      "Epoch 5/5, Loss: 0.44235386451085407, Accuracy: 77.19%\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Test Accuracy: 53.05%\n"
     ]
    }
   ],
   "source": [
    "fine_tune_ew(inpainting_encoder, ew_train_loader, ew_test_loader, encoder_in_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.9703331391016642, Accuracy: 36.14%\n",
      "Epoch 2/5, Loss: 0.9385581016540527, Accuracy: 37.89%\n",
      "Epoch 3/5, Loss: 0.9038851857185364, Accuracy: 38.95%\n",
      "Epoch 4/5, Loss: 0.9110100467999777, Accuracy: 40.70%\n",
      "Epoch 5/5, Loss: 0.8556139866511027, Accuracy: 40.00%\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Test Accuracy: 52.22%\n"
     ]
    }
   ],
   "source": [
    "fine_tune_ew(mae_encoder, ew_train_loader, ew_test_loader, encoder_in_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HDDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
